This project explores how Agentic AI can be applied to a real enterprise security problem: insider threat detection. In most organizations, security data exists across dozens of systems—authentication logs, access records, permission changes, and alerts—yet investigating a single incident still requires manual querying, dashboard hopping, and human correlation. We wanted to change that by building an AI agent that can reason across security data the way a human analyst would, but at machine speed.

Instead of building another dashboard or rule-based system, we designed an AI-powered security assistant that understands enterprise security context and can perform multi-step investigations autonomously. Using natural language, a security analyst can ask questions like “Are there any impossible travel patterns in the last 24 hours?” or “Investigate employee EMP014,” and the agent figures out what data it needs, queries the database safely, and produces a structured, actionable assessment. This makes the system immediately usable by non-technical stakeholders while still grounded in real data.

The core idea behind this project is agentic reasoning. The AI model does not have direct access to the database. Instead, it operates through the Model Context Protocol (MCP), which acts as a controlled gateway between the agent and PostgreSQL. This separation is intentional and security-focused: the agent can only execute explicitly allowed queries, ensuring safety, auditability, and trust. The database itself is designed to resemble a real enterprise environment, complete with employees, roles, systems, authentication events, access logs, data activity, permission changes, alerts, and even audit-tampering detection through triggers.

What makes this project compelling is that the agent is not answering pre-scripted questions. For complex prompts, it decomposes the request into multiple sub-queries, correlates information across tables, and reasons about timelines, severity, access patterns, and policy violations. For example, when investigating a potentially malicious employee, the agent examines authentication locations, privilege escalations, after-hours access, data download volumes, and existing alerts before producing an executive-style risk summary and recommended actions.

From a technical perspective, the project uses PostgreSQL as the source of truth, the MCP PostgreSQL server as the secure data interface, and Archestra as the agent orchestration layer. Docker is used to manage services cleanly and reproducibly. The focus throughout the implementation was not on flashy UI, but on correctness, safety, and reasoning quality—exactly what matters in enterprise AI systems.

Building this project was a strong learning experience in designing agentic systems the right way. We learned how critical schema clarity is when working with LLMs, how easily hallucinations can occur if structure is ambiguous, and how powerful an AI agent becomes when it is tightly grounded in real data. More importantly, we gained hands-on experience with patterns that are increasingly relevant in industry: tool-using agents, secure AI-to-database communication, and multi-step autonomous reasoning.

This project represents our first serious step into Agentic AI, and it reflects how we believe AI systems should be built for the real world—not as standalone chatbots, but as intelligent agents that operate within clearly defined boundaries, reason across complex systems, and deliver practical value.
